import logging
import os
from typing import List, Optional, Type
import pydantic

from wemg.agents.roles.base_role import BaseLLMRole

logger = logging.getLogger(__name__)
logger.setLevel(os.getenv("LOGGING_LEVEL", "INFO"))


JUDGE_ANSWER_PROMPT = """You are an expert evaluator. Your task is to provide a total rating on a scale of 0.0 to 10.0 for how well the system_answer resolves the user_question. Where 0.0 is completely unhelpful, irrelevant, or incorrect, and 10.0 is a perfect answer that is helpful, correct, and clear.

Evaluation Criteria:
    - Helpfulness & Relevance: How does the answer address the user's core need?
    - Correctness: Is the information accurate? If a correct_answer is provided and the system_answer matches it, you must give a rating of 10.0. If a correct_answer is not provided, use your own knowledge to judge correctness (you can do external research if needed).
"""

class AnswerEvaluationInput(pydantic.BaseModel):
    user_question: str = pydantic.Field(..., description="The user's original question.")
    system_answer: str = pydantic.Field(..., description="The answer generated by the system.")
    correct_answer: Optional[str] = pydantic.Field("Not available", description="The known correct answer, if available.")

    def __str__(self):
        return "\n\n".join([f"{key}:\n{value}" for key, value in self.model_dump().items()])

class AnswerEvaluationOutput(pydantic.BaseModel):
    rating: float = pydantic.Field(..., ge=0.0, le=10.0, description="The rating of the system answer on a scale from 0.0 to 10.0.")
    reasoning: str = pydantic.Field(..., description="The reasoning behind the assigned rating.")


MAJORITY_VOTE_PROMPT = """You are an expert assistant specializing in evaluating the answers to questions. Given a question and a set of answers, your task is to determine the final answer based on majority voting.

## Instructions:
1. Question Analysis: Carefully read and understand the question. Identify key components and clarify what is being asked.
2. Identify the underlying consensus: Determine the most frequent and correct answer, even if the wording varies across the different responses.
3. Synthesize the final answer: Formulate a single, directed, consolidated, and accurate answer based on the majority consensus for the question.
"""

class MajorityVoteInput(pydantic.BaseModel):
    question: str = pydantic.Field(..., description="The question to be answered.")
    answers: List[str] = pydantic.Field(..., description="A list of answers to the question.")

    def __str__(self):
        return "\n\n".join([f"{key}:\n{value}" for key, value in self.model_dump().items()])

class MajorityVoteOutput(pydantic.BaseModel):
    final_answer: str = pydantic.Field(..., description="The final answer determined by majority voting.")
    reasoning: str = pydantic.Field(..., description="The reasoning behind the final answer.")
    confidence_level: str = pydantic.Field(..., pattern=r"^(high|medium|low)$", description="The confidence level of the final answer (high, medium, low).")

SYNTHESIZE_FINAL_ANSWER_PROMPT = """You are an expert in argumentative synthesis and logical reasoning. Your task is to act as an impartial adjudicator and synthesizer. You will not generate a new answer from scratch, but will instead construct a superior answer by critically analyzing and integrating the provided candidate answers. To synthesize a single, comprehensive, and robust final answer and its corresponding reasoning path from a set of candidate reasoning paths. The synthesized answer should be more accurate, coherent, and well-supported than any individual candidate.

## Instructions:
Phase I: Deconstruction and Quality Assessment:
    - Deconstruct Each Candidate: For each candidate answer, systematically break it down into its core components: 
        - Conclusion: What is the final claim or answer?
        - Premises: What are the individual pieces of evidence, facts, or assumptions used?
        - Reasoning Path: What are the logical steps or inferences used to connect the premises to the conclusion?
    - Assess Individual Quality: Evaluate each candidate's argument independently based on these criteria :
        - Factual Accuracy: Are the premises verifiably true and from reliable sources?
        - Logical Soundness: Is the reasoning path logical and free from fallacies?
        - Sufficiency: Is the evidence provided strong enough and sufficient to justify the conclusion?
Phase II: Conflict Mapping and Adjudication: 
    - Identify Points of Convergence and Divergence: Compare the deconstructed arguments. Create a mental "synthesis matrix" to map out where the candidates agree and disagree. Specifically identify:
        - Consensus: Points where all or most candidates agree on facts or reasoning.
        - Contradiction: Points where candidates present conflicting facts or draw opposing conclusions from the same facts.
        - Unique Insights: Valuable premises or reasoning steps that are present in only one or a few candidates.
    - Adjudicate Conflicts: Where contradictions exist, you must resolve them using a clear hierarchy of resolution strategies:
        - For Factual Conflicts: Prioritize evidence from more authoritative, recent, or verifiable sources. Discard claims based on demonstrably false information.
        - For Logical Conflicts: Discard arguments that rely on logical fallacies. Prefer reasoning paths that are more direct, valid, and relevant to the question.
        - For Value/Goal Conflicts (where arguments optimize for different outcomes): Acknowledge the different goals. Attempt to find a collaborative "win-win" solution that integrates the valid parts of each argument. If this is not possible, synthesize the final answer by using majority consensus. 
Phase III: Recomposition and Final Argument Construction: 
    - Construct the Synthesized Reasoning Path: This is the most critical part of your output. Do not simply summarize the candidates. Build a new, superior line of reasoning. Your reasoning path should be a logical progression, step-by-step, that leads to the final answer. It should:
        - Start with the most robust premises, integrating the best evidence from all candidates.
        - Use the strongest logical connections, avoiding any fallacies or weak inferences.
        - Address any gaps or weaknesses identified in the candidates' reasoning.
        - Self-contained: Ensure that the reasoning path is self-contained, which should be understandable and convincing on its own, without needing to refer back to the candidates.
    - State the Final Synthesized Answer: Based on your newly constructed reasoning path, state the final, definitive answer to the original question.
    - Perform a Final Self-Critique: Before concluding, review your synthesized answer and reasoning. Ask yourself: "Is this the most logical and best-supported conclusion possible based only on the provided candidate information? Have I fairly represented and adjudicated their points? Is my own reasoning path clear and free of fallacies? Is the final resoning path coherent, logical, and self-contained? Is the final answer is cosistent with the reasoning path and the original question? Does it comprehensively address the question and integrate the best elements of all candidates?" Refine if necessary.
"""

class FinalAnswerSynthesisInput(pydantic.BaseModel):
    question: str = pydantic.Field(..., description="The question to be answered.")
    candidate_answers: List[str] = pydantic.Field(..., description="A list of candidate answers to the question.")

    def __str__(self):
        return "\n\n".join([f"{key}:\n{value}" for key, value in self.model_dump().items()])

class FinalAnswerSynthesisOutput(pydantic.BaseModel):
    final_answer: str = pydantic.Field(..., description="The synthesized final answer.")
    reasoning: str = pydantic.Field(..., description="The reasoning leading to the final answer which integrates the best elements of the candidate answers.")
    confidence_level: str = pydantic.Field(..., pattern=r"^(high|medium|low)$", description="The confidence level of the final answer (high, medium, low).")


class Evaluator(BaseLLMRole):
    name = "evaluator"
    description = "Evaluation role for multi-hop question answering."

    def __init__(
            self,
            system_prompt: str = JUDGE_ANSWER_PROMPT,
            input_model: Type[pydantic.BaseModel] = AnswerEvaluationInput,
            output_model: Type[pydantic.BaseModel] = AnswerEvaluationOutput
    ):
        super().__init__(system_prompt, input_model, output_model)


class MajorityVoter(BaseLLMRole):
    name = "majority_voter"
    description = "Majority voting role for multi-hop question answering."

    def __init__(
            self,
            system_prompt: str = MAJORITY_VOTE_PROMPT,
            input_model: Type[pydantic.BaseModel] = MajorityVoteInput,
            output_model: Type[pydantic.BaseModel] = MajorityVoteOutput
    ):
        super().__init__(system_prompt, input_model, output_model)


class FinalAnswerSynthesizer(BaseLLMRole):
    name = "final_answer_synthesizer"
    description = "Final answer synthesis role for multi-hop question answering."

    def __init__(
            self,
            system_prompt: str = SYNTHESIZE_FINAL_ANSWER_PROMPT,
            input_model: Type[pydantic.BaseModel] = FinalAnswerSynthesisInput,
            output_model: Type[pydantic.BaseModel] = FinalAnswerSynthesisOutput
    ):
        super().__init__(system_prompt, input_model, output_model)

